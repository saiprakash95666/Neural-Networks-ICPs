{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Sentiment Analysis for Twitter data"
      ],
      "metadata": {
        "id": "tX9N-xnmaZYy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
        "from matplotlib import pyplot\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.utils import to_categorical\n",
        "import re\n",
        "from sklearn.preprocessing import LabelEncoder"
      ],
      "metadata": {
        "id": "YDXwKmU0bL7f"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "csUKnEP4aTs2",
        "outputId": "b74bbddb-fc9c-44af-a3f3-913a21a3703d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "291/291 - 29s - 98ms/step - accuracy: 0.6383 - loss: 0.8373\n",
            "144/144 - 3s - 22ms/step - accuracy: 0.6651 - loss: 0.7659\n",
            "0.7658703923225403\n",
            "0.6651375889778137\n",
            "['loss', 'compile_metrics']\n"
          ]
        }
      ],
      "source": [
        "data = pd.read_csv('/content/Sentiment.csv')\n",
        "# Keeping only the neccessary columns\n",
        "data = data[['text','sentiment']]\n",
        "\n",
        "data['text'] = data['text'].apply(lambda x: x.lower())\n",
        "data['text'] = data['text'].apply((lambda x: re.sub('[^a-zA-z0-9\\s]', '', x)))\n",
        "\n",
        "for idx, row in data.iterrows():\n",
        "    row[0] = row[0].replace('rt', ' ')\n",
        "\n",
        "max_features = 2000\n",
        "tokenizer = Tokenizer(num_words=max_features, split=' ')\n",
        "tokenizer.fit_on_texts(data['text'].values)\n",
        "X = tokenizer.texts_to_sequences(data['text'].values)\n",
        "\n",
        "X = pad_sequences(X)\n",
        "\n",
        "embed_dim = 128\n",
        "lstm_out = 196\n",
        "def createmodel():\n",
        "    model = Sequential()\n",
        "    model.add(Embedding(max_features, embed_dim,input_length = X.shape[1]))\n",
        "    model.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\n",
        "    model.add(Dense(3,activation='softmax'))\n",
        "    model.compile(loss = 'categorical_crossentropy', optimizer='adam',metrics = ['accuracy'])\n",
        "    return model\n",
        "# print(model.summary())\n",
        "\n",
        "labelencoder = LabelEncoder()\n",
        "integer_encoded = labelencoder.fit_transform(data['sentiment'])\n",
        "y = to_categorical(integer_encoded)\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X,y, test_size = 0.33, random_state = 42)\n",
        "\n",
        "batch_size = 32\n",
        "model = createmodel()\n",
        "model.fit(X_train, Y_train, epochs = 1, batch_size=batch_size, verbose = 2)\n",
        "score,acc = model.evaluate(X_test,Y_test,verbose=2,batch_size=batch_size)\n",
        "print(score)\n",
        "print(acc)\n",
        "print(model.metrics_names)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.save('SentimentAnalysis.h5')"
      ],
      "metadata": {
        "id": "kYWzz9tTc11j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7251bb36-5fbf-433f-f0b2-05d18d3340d0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tweepy\n",
        "from keras.models import load_model\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import re"
      ],
      "metadata": {
        "id": "07-JzJpQf1Re"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the saved model\n",
        "model = load_model('/content/SentimentAnalysis.h5')\n",
        "\n",
        "# Define a function for preprocessing text\n",
        "def preprocess_data(text):\n",
        "  text = text.lower()\n",
        "  text = re.sub('[^a-zA-z0-9\\s]', '', text)\n",
        "  return text\n",
        "\n",
        "new_data = \"A lot of good things are happening. We are respected again throughout the world, and that's a great thing\"\n",
        "# Preprocess the new text data\n",
        "new_data = preprocess_data(new_data)\n",
        "\n",
        "# Tokenize and pad the new text data\n",
        "max_features = 2000\n",
        "tokenizer = Tokenizer(num_words=max_features, split=' ')\n",
        "tokenizer.fit_on_texts([new_data])\n",
        "X_new = tokenizer.texts_to_sequences([new_data])\n",
        "X_new = pad_sequences(X_new, maxlen=model.input_shape[1])\n",
        "\n",
        "# Make predictions\n",
        "predictions = model.predict(X_new)\n",
        "\n",
        "# Determine the sentiment based on the prediction\n",
        "sentiments = ['Negative', 'Neutral', 'Positive']\n",
        "predicted_sentiment = sentiments[predictions.argmax()]\n",
        "\n",
        "# Print the result\n",
        "print(\"Predicted Sentiment: \" + predicted_sentiment)"
      ],
      "metadata": {
        "id": "4rSCUiEndrv3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f41aa3b-2cfa-4c90-c0f4-b0e7680d17f4"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 244ms/step\n",
            "Predicted Sentiment: Negative\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apply GridSearchCV on the source code"
      ],
      "metadata": {
        "id": "TXMVAAn3CfVB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install scikeras\n",
        "from scikeras.wrappers import KerasClassifier"
      ],
      "metadata": {
        "id": "h57eQCf_CqkY"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import pandas as pd\n",
        "import re\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Embedding, LSTM, SpatialDropout1D\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from scikeras.wrappers import KerasClassifier\n",
        "\n",
        "# Assuming the data loading and preprocessing steps are the same\n",
        "model = load_model('/content/SentimentAnalysis.h5')\n",
        "max_features = 2000\n",
        "tokenizer = Tokenizer(num_words=max_features, split=' ')\n",
        "# Assuming tokenizer fitting and text preprocessing is done here\n",
        "\n",
        "def createmodel(optimizer='adam'):\n",
        "    model1 = Sequential()\n",
        "    model1.add(Embedding(max_features, embed_dim, input_length=X.shape[1]))\n",
        "    model1.add(SpatialDropout1D(0.2))\n",
        "    model1.add(LSTM(lstm_out, dropout=0.2, recurrent_dropout=0.2))\n",
        "    model1.add(Dense(3, activation='softmax'))\n",
        "    model1.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
        "    return model\n",
        "\n",
        "# Define the KerasClassifier with the build_fn as our model creation function\n",
        "model = KerasClassifier(model, verbose=2)\n",
        "\n",
        "# Define hyperparameters to tune\n",
        "param_grid = {\n",
        "    'batch_size': [32, 64],\n",
        "    'epochs': [1, 2],\n",
        "    'optimizer': ['adam', 'rmsprop']\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nG_0dFChDf0u",
        "outputId": "22bf2607-64f4-43a3-fd0c-f534a05527a7"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Initialize GridSearchCV\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, n_jobs=1, cv=3)\n"
      ],
      "metadata": {
        "id": "vm5T8RxvDwXP"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fit GridSearchCV\n",
        "grid_result = grid.fit(X_train, Y_train)\n",
        "\n",
        "# Summarize results\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3El5fSP3Le2x",
        "outputId": "2ad68a7f-ecda-498d-cd55-53b69fd85894"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/saving/saving_lib.py:576: UserWarning: Skipping variable loading for optimizer 'adam', because it has 14 variables whereas the saved optimizer has 2 variables. \n",
            "  saveable.load_own_variables(weights_store.get(inner_path))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "194/194 - 16s - 82ms/step - accuracy: 0.7080 - loss: 0.6942\n",
            "97/97 - 2s - 25ms/step\n",
            "194/194 - 16s - 83ms/step - accuracy: 0.7106 - loss: 0.6834\n",
            "97/97 - 2s - 25ms/step\n",
            "194/194 - 16s - 80ms/step - accuracy: 0.7079 - loss: 0.6864\n",
            "97/97 - 2s - 24ms/step\n",
            "194/194 - 16s - 81ms/step - accuracy: 0.7052 - loss: 0.6961\n",
            "97/97 - 2s - 25ms/step\n",
            "194/194 - 16s - 84ms/step - accuracy: 0.7070 - loss: 0.6809\n",
            "97/97 - 3s - 32ms/step\n",
            "194/194 - 16s - 82ms/step - accuracy: 0.7055 - loss: 0.6821\n",
            "97/97 - 4s - 38ms/step\n",
            "Epoch 1/2\n",
            "194/194 - 16s - 81ms/step - accuracy: 0.7035 - loss: 0.6947\n",
            "Epoch 2/2\n",
            "194/194 - 20s - 105ms/step - accuracy: 0.7406 - loss: 0.6278\n",
            "97/97 - 4s - 36ms/step\n",
            "Epoch 1/2\n",
            "194/194 - 16s - 85ms/step - accuracy: 0.7075 - loss: 0.6839\n",
            "Epoch 2/2\n",
            "194/194 - 20s - 104ms/step - accuracy: 0.7422 - loss: 0.6152\n",
            "97/97 - 3s - 36ms/step\n",
            "Epoch 1/2\n",
            "194/194 - 17s - 89ms/step - accuracy: 0.7080 - loss: 0.6787\n",
            "Epoch 2/2\n",
            "194/194 - 13s - 68ms/step - accuracy: 0.7456 - loss: 0.6094\n",
            "97/97 - 3s - 28ms/step\n",
            "Epoch 1/2\n",
            "194/194 - 16s - 82ms/step - accuracy: 0.7035 - loss: 0.6932\n",
            "Epoch 2/2\n",
            "194/194 - 21s - 107ms/step - accuracy: 0.7408 - loss: 0.6287\n",
            "97/97 - 2s - 24ms/step\n",
            "Epoch 1/2\n",
            "194/194 - 16s - 81ms/step - accuracy: 0.7122 - loss: 0.6855\n",
            "Epoch 2/2\n",
            "194/194 - 14s - 71ms/step - accuracy: 0.7425 - loss: 0.6163\n",
            "97/97 - 2s - 24ms/step\n",
            "Epoch 1/2\n",
            "194/194 - 16s - 84ms/step - accuracy: 0.7076 - loss: 0.6836\n",
            "Epoch 2/2\n",
            "194/194 - 21s - 107ms/step - accuracy: 0.7481 - loss: 0.5998\n",
            "97/97 - 2s - 24ms/step\n",
            "97/97 - 13s - 138ms/step - accuracy: 0.7009 - loss: 0.6964\n",
            "49/49 - 3s - 61ms/step\n",
            "97/97 - 11s - 112ms/step - accuracy: 0.7069 - loss: 0.6854\n",
            "49/49 - 2s - 38ms/step\n",
            "97/97 - 12s - 124ms/step - accuracy: 0.7114 - loss: 0.6758\n",
            "49/49 - 2s - 40ms/step\n",
            "97/97 - 12s - 128ms/step - accuracy: 0.7085 - loss: 0.6866\n",
            "49/49 - 2s - 39ms/step\n",
            "97/97 - 12s - 125ms/step - accuracy: 0.7109 - loss: 0.6779\n",
            "49/49 - 2s - 40ms/step\n",
            "97/97 - 13s - 132ms/step - accuracy: 0.7103 - loss: 0.6796\n",
            "49/49 - 2s - 39ms/step\n",
            "Epoch 1/2\n",
            "97/97 - 12s - 123ms/step - accuracy: 0.7052 - loss: 0.6871\n",
            "Epoch 2/2\n",
            "97/97 - 10s - 102ms/step - accuracy: 0.7425 - loss: 0.6256\n",
            "49/49 - 2s - 40ms/step\n",
            "Epoch 1/2\n",
            "97/97 - 12s - 124ms/step - accuracy: 0.7090 - loss: 0.6786\n",
            "Epoch 2/2\n",
            "97/97 - 10s - 102ms/step - accuracy: 0.7391 - loss: 0.6198\n",
            "49/49 - 2s - 41ms/step\n",
            "Epoch 1/2\n",
            "97/97 - 12s - 125ms/step - accuracy: 0.7155 - loss: 0.6788\n",
            "Epoch 2/2\n",
            "97/97 - 8s - 87ms/step - accuracy: 0.7400 - loss: 0.6035\n",
            "49/49 - 2s - 50ms/step\n",
            "Epoch 1/2\n",
            "97/97 - 14s - 140ms/step - accuracy: 0.7117 - loss: 0.6884\n",
            "Epoch 2/2\n",
            "97/97 - 18s - 188ms/step - accuracy: 0.7424 - loss: 0.6190\n",
            "49/49 - 2s - 39ms/step\n",
            "Epoch 1/2\n",
            "97/97 - 12s - 125ms/step - accuracy: 0.7090 - loss: 0.6784\n",
            "Epoch 2/2\n",
            "97/97 - 12s - 120ms/step - accuracy: 0.7462 - loss: 0.6108\n",
            "49/49 - 2s - 42ms/step\n",
            "Epoch 1/2\n",
            "97/97 - 12s - 125ms/step - accuracy: 0.7143 - loss: 0.6784\n",
            "Epoch 2/2\n",
            "97/97 - 11s - 117ms/step - accuracy: 0.7424 - loss: 0.6114\n",
            "49/49 - 2s - 38ms/step\n",
            "146/146 - 17s - 117ms/step - accuracy: 0.7083 - loss: 0.6834\n",
            "Best: 0.716346 using {'batch_size': 64, 'epochs': 1, 'optimizer': 'adam'}\n"
          ]
        }
      ]
    }
  ]
}